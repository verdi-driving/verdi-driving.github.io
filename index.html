<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>VERDI: VLM-Embedded Reasoning for Autonomous Driving</title>

  <!--  =====  FONTS & ICONS  =====  -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" />
  <link rel="icon" href="./static/icon.png" /> <!-- TODO: page icon -->

  <!--  =====  CSS  =====  -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick-theme.min.css" />

  <style>
    /* --- GLOBAL --- */
    body{background:#fff;font-family:"Noto Sans",sans-serif;font-size:18px; line-height:1.5;color:#333;}
    section{padding:2.5rem 0;}

    /* --- SECTION COLORS --- */
    .section-gray{background:#f7f7f7;}

    /* --- TYPOGRAPHY & LINKS --- */
    .task-title{margin-bottom:1rem;font-weight:600;}
    .publication-authors{margin-bottom:1rem;} /* space below authors */
    .publication-authors a{color:#3273dc;text-decoration:none;white-space:nowrap;}
    .publication-authors a:hover{text-decoration:underline;}

    /* --- HERO & SPACING TWEAKS --- */
    .hero{padding-bottom:0.0rem;} /* tighter gap above overview */
    #overview.section{padding-top:0.0rem;} /* tighter gap below hero */
    .publication-links{margin-top:1.5rem;margin-bottom:1.5rem;} /* gap between authors/icons & icons/logo */
    figure.lab-logo{margin-top:1.5rem;} /* gap between icons and Princeton logo */
    /* #sim-robot.section{padding-bottom:0.1rem;} gap above simulation section */
    #bibtex.section-{padding-top:0rem;} /* gap above BibTeX section */

    /* --- SLICK ARROWS --- */
    .slick-prev:before,.slick-next:before{color:#3273dc;font-size:32px;}

    /* --- THUMBNAILS --- */
    .video-thumb{cursor:pointer;border-radius:8px;overflow:hidden;box-shadow:0 2px 4px rgba(0,0,0,0.1);margin:0 8px;}
    .video-thumb video{width:100%;height:auto;display:block;}

    /* --- STAND‑ALONE VIDEOS --- */
    .overview-video,.sim-video{width:100%;height:auto;border-radius:0;box-shadow:none;pointer-events:none;}

    /* --- LAYOUT HELPERS --- */
    .task-block{margin-bottom:3rem;}
    .task-carousel{margin-top:1rem;}

    /* --- CONTENT WIDTH CONSISTENCY --- */
    .content-container{max-width:960px;margin:0 auto;}

    /* --- PDF EMBED --- */
    .pdf-container{margin-top:1.5rem;}
    .pdf-container object{width:100%;height:600px;border:none;}

    /* --- REAL EXP IMG --- */
    .real-exp-img{width:100%;height:auto;max-width:100%;}
    /* --- LINK COLOR (lighter blue, closer to default hyperlinks) --- */

    .publication-authors a:hover{
      text-decoration:underline;
    }

    /* --- HERO ↔ OVERVIEW GAP (shrink) --- */
    .hero{padding-bottom:0rem;}         /* was 0.5rem */
    #overview.section{padding-top:0rem;}/* was 0.75rem */

    /* --- REAL-ROBOT ↔ SIMULATION GAP (shrink) --- */
    #real-robot.section{padding-top:2.5rem;} /* default Bulma ≈2.5rem */
    #real-robot.section{padding-bottom:2.5rem;} /* default Bulma ≈2.5rem */
    #sim-robot.section {padding-top:2.5rem;}    /* default Bulma ≈2.5rem */

    /* --- SIMULATION ↔ BIBTEX GAP (shrink) --- */
    #BibTeX.section{padding-top:0.1rem;}        /* tighten above BibTeX */
  </style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script>
    window.addEventListener('load', () => {
      if (window.MathJax) {
        MathJax.typesetPromise();
      }
    });
  </script>
  

</head>
<body>

  <!-- ===== HERO with title & authors ===== -->
  <section class="hero section">
    <div class="hero-body">
      <div class="container is-max-widescreen has-text-centered">
        <h1 class="title is-2 publication-title">VERDI: VLM-Embedded Reasoning<br>for Autonomous Driving</h1>
        <!-- ===== AUTHORS (three rows, no underscores) ===== -->
        <div class="is-size-5 publication-authors">
          <div>
            <span class="author-block"><a href="https://fengb2-coder.github.io/">Bowen&nbsp;Feng*</a><sup>1</sup></span>,
            <span class="author-block"><a href="https://may0mei.github.io/">Zhiting&nbsp;Mei*</a><sup>2</sup></span>,
            <span class="author-block"><a href="https://ztmotalee.github.io/">Baiang&nbsp;Li</a><sup>1</sup></span>,
            <span class="author-block"><a href="https://www.ostjul.com/">Julian&nbsp;Ost</a><sup>1</sup></span>,
            <span class="author-block"><a href="https://ca.linkedin.com/in/roger-girgis-a46b959b">Roger&nbsp;Girgis</a><sup>3</sup></span>,
            <span class="author-block"><a href="https://irom-lab.princeton.edu/majumdar/">Anirudha&nbsp;Majumdar</a><sup>2</sup></span>,
            <span class="author-block"><a href="https://www.cs.princeton.edu/~fheide/">Felix&nbsp;Heide</a><sup>1,3</sup></span>
          </div>
        </div>

        <div class="is-size-5 publication-authors">
          <span class="author-block"><sup>1</sup><a href="https://light.princeton.edu/">Princeton Computational Imaging Lab</a>,</span>
          <span class="author-block"><sup>2</sup><a href="https://irom-lab.princeton.edu/">Intelligent Robot Motion Lab</a>,</span>
          <span class="author-block"><sup>3</sup><a href="https://torc.ai/">Torc Robotics</a></span>
        </div>

        <!-- ===== RESOURCE ICONS ===== -->
        <div class="publication-links">
          <a href="static/paper.pdf" class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener">
            <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
          </a>
          <a href="https://arxiv.org/abs/2505.15925" class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener">
            <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
          </a>
          <a href="#" class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener">
            <span class="icon"><i class="fab fa-github"></i></span><span>Code (Coming Soon!)</span>
          </a>
          <!-- <a href="#" class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener"> TODO: fix video link -->
            <!-- <span class="icon"><i class="fab fa-youtube"></i></span><span>Video</span> -->
          <!-- </a> -->
        </div>

        <!-- ===== LAB LOGO ===== -->
        <!--<figure class="image is-inline-block lab-logo">
          <img src="static/pci-icon.png" alt="PCI Lab logo" style="max-width:150px;">
        </figure>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <figure class="image is-inline-block lab-logo">
          <img src="static/irom-icon.png" alt="IROM Lab logo" style="max-width:150px;">
        </figure>-->
      </div>
    </div>
  </section>

  <!-- ===== OVERVIEW ===== -->
  <section id="overview" class="section">
    <div class="container is-max-desktop content-container has-text-centered" style="margin-top:-3rem">
      <video class="overview-video" autoplay loop muted playsinline poster="static/anchor.png">
        <source src="static/Anchor.mov" type="video/mp4" />
        Your browser does not support the video tag.
      </video>
      <p style="margin-top:0rem;">In autonomous driving, we aim to achieve both (1) <strong>fast online planning</strong> with a modularized differentiable end-to-end (e2e) architecture, and (2) <strong>human-like reasoning process</strong> with a vision-language model (VLM). Our key idea is to distill the reasoning process and commonsense knowledge from a VLM to the e2e driving model.</p>
    </div>
  </section>

  <!-- ===== ABSTRACT ===== -->
  <section id="abstract" class="section">
    <div class="container is-max-desktop content-container">
      <h2 class="title is-3 has-text-centered">Abstract</h2>
      <p>
        While autonomous driving (AD) stacks struggle with decision making under partial observability and real-world complexity, human drivers are capable of applying commonsense reasoning to make near-optimal decisions with limited information. Recent work has attempted to leverage finetuned Vision-Language Models (VLMs) for trajectory planning at inference time to emulate human behavior, but the long inference time makes them impractical to deploy. To bridge this gap, we propose <strong>V</strong>LM-<strong>E</strong>mbedded <strong>R</strong>easoning for autonomous <strong>D</strong>r<strong>I</strong>ving (<strong>VERDI</strong>), a training-time framework that distills the reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI augments modular differentiable end-to-end (e2e) AD models by aligning intermediate module outputs at the perception, prediction, and planning stages with text features explaining the driving reasoning process produced by VLMs. We demonstrate the effectiveness of our method on the NuScenes dataset and find that VERDI outperforms existing e2e methods that do not embed reasoning by 10% in \( \ell_{2} \) distance, while maintaining high inference speed.
      </p>
      <br>
      <video id="summary-video" class="shadow" controls preload="metadata" width="100%" poster="static/video.png">
        <source src="static/video.mp4" type="video/mp4" />
        Your browser does not support the video tag.
      </video>
    </div>
  </section>

<!-- ==== Approach ==== -->
<section id="approach" class="section">
  <div class="container is-max-desktop content-container">
    <h2 class="title is-3 has-text-centered">Approach</h2>
    <p style="margin-top:0rem;">We align language features acquired from the VLM reasoning module with driving features acquired from the e2e driving model. Specifically, we do so for each of the perception, prediction, and planning submodules to effectively embed the VLM reasoning process into the e2e model.</p>
  </div>
</section>

<!-- ==== Vlm Reasoning Module (Side-by-Side) ==== -->
<section id="vlm-reasoning" class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-4 has-text-centered">VLM Reasoning Module</h2>
    <div class="columns is-vcentered">
      <div class="column is-4 content-container">
        <p>To acquire language features, we use the ground truth trajectory to query a VLM for the reasoning process behind the driving behavior.
          We ask in the order of perception, prediction, and planning, about other agents and objects in spatial order, other agents' behavior, and the ego vehicle's behavior.
          After obtaining textual responses from the VLM, we map them to the latent space with a language encoder.</p>
      </div>
      <div class="column is-8">
        <figure class="image">
          <img src="static/prompt.png" alt="VLM Reasoning Diagram" style="width: 100%; max-width: 750px; margin: 0 auto;">
        </figure>
      </div>
    </div>
    <!-- <p style="margin: top -10px;">For planning, we provide answers from preceding modules, and ask the VLM to choose from a comprehensive list of actions. After obtaining textual responses from the VLM, we map them to the latent space with a language encoder.</p> -->
  </div>
</section>


<!-- ==== Vlm Reasoning Texts ==== -->
<!-- <section id="vlm reasoning" class="section">
  <div class="container is-max-desktop content-container has-text-centered">
    <h2 class="title is-4 has-text-centered">VLM Reasoning Module</h3>
    <p style="margin-top:0rem;">To acquire language features, we use the ground truth trajectory to query a VLM for the reasoning process behind the driving behavior.

      For perception, we ask the VLM to label agents and objects from left to right and front to back to avoid spatial confusion or redundancy. For prediction, we provide the perception answer and ask the VLM to answer in the order the agents were listed. For planning, we provide answers from preceding modules, and ask the VLM to choose from a comprehensive list of actions.
      
      After obtaining textual responses from the VLM, we map them to the latent space with a language encoder.</p>
  </div>
</section> -->

<!-- ==== VLM Prompt Image ==== -->
<!-- <section class="section">
  <div class="container is-max-desktop">
    <figure class="image">
      <img src="static/prompt.png" alt="VLM Reasoning Diagram">
    </figure>
  </div>
</section> -->

<!-- ==== Driving feature ==== -->
<section id="e2e" class="section">
  <div class="container is-max-desktop content-container">
    <h2 class="title is-4 has-text-centered">Aligning with the e2e Model</h3>
    <p style="margin-top:0rem;">To obtain driving features, we introduce a learnable Progressive Feature Projector (PFP) to each of the perception, prediction, and planning submodules. The PFPs map driving features to the same latent space as the language features, and align them with a cosine similarity loss.</p>
  </div>
  <div class="container is-max-desktop">
    <video class="overview-video" autoplay loop muted playsinline poster="static/e2e.png">
      <source src="static/e2e.mov" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
  </div>
</section>


<!-- ===== Results video ==== -->
<section id="results" class="section">
  <div class="container is-max-desktop content-container">
    <h2 class="title is-3 has-text-centered">Experiments</h2>
    <video class="overview-video" autoplay loop muted playsinline poster="static/results.png">
      <source src="static/results.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
    <p style="margin-top:0rem;">We show superior performance against our direct baseline <strong>VAD</strong>, a supervised e2e model without embedded reasoning. The video shows the multi-view camera observations on the left and the BEV view on the right. Each of the example shows our successful performance on the <strong>perception, prediction, and planning</strong> modules. We also show the VLM text response for each testing case to demonstrate that VERDI has successfully distilled VLM's reasoning and commonsense knowledge.</p>
  </div>
</section>


<!-- ===== Table of results ===== -->
<style>
  table {
    border-collapse: collapse;
    max-width: 750px;
    margin: 20px auto;
    font-family: sans-serif;
    font-size: 13px;
    border: 0px solid black;
  }
  th, td {
    border: 0px solid black;
    padding: 4px 8px;
    text-align: center;
  }
  th {
    font-weight: bold;
    white-space: nowrap;
  }
  td:first-child, th:first-child {
    text-align: left;
  }

  /* Simulate toprule, midrule, bottomrule */
  thead tr:first-child {
    border-top: 2px solid black;   /* Toprule */
    border-bottom: 1px solid black; /* Midrule */
  }
  tbody tr:last-child {
    border-bottom: 2px solid black; /* Bottomrule */
  }
  /* tbody tr:not(:last-child) {
    border-bottom: 1px solid #ccc; Light midrules between rows  } */

  /* Line between OpenEMMA and OmniDrive */
    .midrule {
    border-top: 1px solid black;
  }
  .bold {
    font-weight: bold;
  }
</style>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Requires VLM<br>@ Inference</th>
      <th>FPS \( \uparrow \)</th>
      <th><em>\( \ell_2 \)</em> (1s) \( \downarrow \)</th>
      <th><em>\( \ell_2 \)</em> (2s) \( \downarrow \)</th>
      <th><em>\( \ell_2 \)</em> (3s) \( \downarrow \)</th>
      <th><em>\( \ell_2 \)</em> (avg.) \( \downarrow \)</th>
      <th>Ego Status</th>
    </tr>
  </thead>
  <tr>
    <td><a href="https://tsinghua-mars-lab.github.io/DriveVLM/">DriveVLM</a></td>
    <td>\( \checkmark \)</td>
    <td>2.43</td>
    <td>0.18</td>
    <td>0.34</td>
    <td>0.68</td>
    <td>0.40</td>
    <td>\( \checkmark \)</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2412.15208">OpenEMMA</a></td>
    <td>\( \checkmark \)</td>
    <td>NA</td>
    <td>1.45</td>
    <td>3.21</td>
    <td>3.76</td>
    <td>2.81</td>
    <td>\( \checkmark \)</td>
  </tr>
  <tr class="midrule">
    <td><a href="https://arxiv.org/abs/2405.01533">OmniDrive</a></span></td>
    <td>\( \checkmark \)</td>
    <td>0.44</td>
    <td>1.15</td>
    <td>1.96</td>
    <td>2.84</td>
    <td>1.98</td>
    <td>-</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2212.10156">UniAD</a></span></td>
    <td>-</td>
    <td>1.8</td>
    <td>0.48</td>
    <td>0.96</td>
    <td>1.05</td>
    <td>0.83</td>
    <td>-</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/2303.12077">VAD-Base</a></span></td>
    <td>-</td>
    <td class="bold">4.5</td>
    <td>0.41</td>
    <td>0.70</td>
    <td>1.05</td>
    <td>0.72</td>
    <td>-</td>
  </tr>
  <tr>
    <td><strong>VERDI</strong></td>
    <td>-</td>
    <td class="bold">4.5</td>
    <td class="bold">0.36</td>
    <td class="bold">0.62</td>
    <td class="bold">0.96</td>
    <td class="bold">0.65</td>
    <td>-</td>
  </tr>
</table>

<section id="results" class="section">
  <div class="container is-max-desktop content-container">
    <p style="margin-top:0rem;">Our method performs <strong>10%</strong> better than the direct baseline VAD. Methods are compared according to: (1) Whether a VLM is required at inference; (2) Inference speed (<strong>FPS</strong>); (3) <strong>Trajectory accuracy</strong>, measured as the \( \ell_{2} \) distance to the expert trajectory at 1s, 2s, and 3s horizon; and (4) Whether precise historical <strong>ego-vehicle state</strong> is used in planning. In a fair comparison with methods not having privileged access to ego status, including location, VERDI achieves the best performance across all metrics.</p>
  </div>
</section>

<!-- ===== BIBTEX ===== -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{feng2025verdivlmembeddedreasoningautonomous,
        title={VERDI: VLM-Embedded Reasoning for Autonomous Driving}, 
        author={Bowen Feng and Zhiting Mei and Baiang Li and Julian Ost and Roger Girgis and Anirudha Majumdar and Felix Heide},
        year={2025},
        eprint={2505.15925},
        archivePrefix={arXiv},
        primaryClass={cs.RO},
        url={https://arxiv.org/abs/2505.15925}, 
  }</code></pre>
    </div>
  </section>
  <br>
  <center class="is-size-10">
    The website design was adapted from <a href="https://nerfies.github.io" class="external-link"><span
        class="dnerf">Nerfies</span></a>.
  </center>
  <br>

  <!-- ===== SCRIPTS ===== -->
  <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.js"></script>
  <script>
    $(function(){
      // Init carousels (only those still using .task-carousel wrappers)
      $('.task-carousel').slick({
        slidesToShow:4,
        slidesToScroll:1,
        infinite:false,
        arrows:true,
        dots:false,
        lazyLoad:'ondemand',
        touchMove:true,
        responsive:[
          {breakpoint:1024,settings:{slidesToShow:3}},
          {breakpoint:768, settings:{slidesToShow:2}},
          {breakpoint:480, settings:{slidesToShow:1}}
        ]
      });

      // Horizontal track‑pad scroll → carousel navigation; let vertical scroll bubble up
      $('.task-carousel').on('wheel',function(e){
        const deltaX = e.originalEvent.deltaX;
        const deltaY = e.originalEvent.deltaY;
        if(Math.abs(deltaX) > Math.abs(deltaY)){
          e.preventDefault();
          $(this).slick(deltaX < 0 ? 'slickPrev' : 'slickNext');
        }
      });
    });
  </script>
</body>
</html>