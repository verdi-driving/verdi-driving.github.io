<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>VERDI: VLM-Embedded Reasoning for Autonomous Driving</title>

  <!--  =====  FONTS & ICONS  =====  -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" />
  <link rel="icon" href="./static/icon.png" /> <!-- TODO: page icon -->

  <!--  =====  CSS  =====  -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick-theme.min.css" />

  <style>
    /* --- GLOBAL --- */
    body{background:#fff;font-family:"Noto Sans",sans-serif;font-size:18px; line-height:1.5;color:#333;}
    section{padding:2.5rem 0;}

    /* --- SECTION COLORS --- */
    .section-gray{background:#f7f7f7;}

    /* --- TYPOGRAPHY & LINKS --- */
    .task-title{margin-bottom:1rem;font-weight:600;}
    .publication-authors{margin-bottom:1rem;} /* space below authors */
    .publication-authors a{color:#3273dc;text-decoration:none;white-space:nowrap;}
    .publication-authors a:hover{text-decoration:underline;}

    /* --- HERO & SPACING TWEAKS --- */
    .hero{padding-bottom:0.5rem;} /* tighter gap above overview */
    #overview.section{padding-top:0.75rem;} /* tighter gap below hero */
    .publication-links{margin-top:1.5rem;margin-bottom:1.5rem;} /* gap between authors/icons & icons/logo */
    figure.lab-logo{margin-top:1.5rem;} /* gap between icons and Princeton logo */
    /* #sim-robot.section{padding-bottom:0.1rem;} gap above simulation section */
    #bibtex.section-{padding-top:0rem;} /* gap above BibTeX section */

    /* --- SLICK ARROWS --- */
    .slick-prev:before,.slick-next:before{color:#3273dc;font-size:32px;}

    /* --- THUMBNAILS --- */
    .video-thumb{cursor:pointer;border-radius:8px;overflow:hidden;box-shadow:0 2px 4px rgba(0,0,0,0.1);margin:0 8px;}
    .video-thumb video{width:100%;height:auto;display:block;}

    /* --- STAND‑ALONE VIDEOS --- */
    .overview-video,.sim-video{width:100%;height:auto;border-radius:0;box-shadow:none;pointer-events:none;}

    /* --- LAYOUT HELPERS --- */
    .task-block{margin-bottom:3rem;}
    .task-carousel{margin-top:1rem;}

    /* --- CONTENT WIDTH CONSISTENCY --- */
    .content-container{max-width:960px;margin:0 auto;}

    /* --- PDF EMBED --- */
    .pdf-container{margin-top:1.5rem;}
    .pdf-container object{width:100%;height:600px;border:none;}

    /* --- REAL EXP IMG --- */
    .real-exp-img{width:100%;height:auto;max-width:100%;}
    /* --- LINK COLOR (lighter blue, closer to default hyperlinks) --- */

    .publication-authors a:hover{
      text-decoration:underline;
    }

    /* --- HERO ↔ OVERVIEW GAP (shrink) --- */
    .hero{padding-bottom:0rem;}         /* was 0.5rem */
    #overview.section{padding-top:0rem;}/* was 0.75rem */

    /* --- REAL-ROBOT ↔ SIMULATION GAP (shrink) --- */
    #real-robot.section{padding-top:2.5rem;} /* default Bulma ≈2.5rem */
    #real-robot.section{padding-bottom:2.5rem;} /* default Bulma ≈2.5rem */
    #sim-robot.section {padding-top:2.5rem;}    /* default Bulma ≈2.5rem */

    /* --- SIMULATION ↔ BIBTEX GAP (shrink) --- */
    #BibTeX.section{padding-top:0.1rem;}        /* tighten above BibTeX */
  </style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script>
    window.addEventListener('load', () => {
      if (window.MathJax) {
        MathJax.typesetPromise();
      }
    });
  </script>
  

</head>
<body>

  <!-- ===== HERO with title & authors ===== -->
  <section class="hero section">
    <div class="hero-body">
      <div class="container is-max-widescreen has-text-centered">
        <h1 class="title is-2 publication-title">VERDI: VLM-Embedded Reasoning<br>for Autonomous Driving</h1>
        <!-- ===== AUTHORS (three rows, no underscores) ===== -->
        <div class="is-size-5 publication-authors">
          <div>
            <span class="author-block"><a href="https://lihzha.github.io/">Bowen&nbsp;Feng*</a></span>, <!-- TODO: update websites ===== -->
            <span class="author-block"><a href="https://abadithela.github.io/">Zhiting&nbsp;Mei*</a></span>,
            <span class="author-block"><a href="https://www.linkedin.com/in/michael-zhang-47a954241/">Julian&nbsp;Ost</a></span>,
            <span class="author-block"><a href="https://jlidard.github.io/">Roger&nbsp;Girgis</a></span>,
            <span class="author-block"><a href="https://irom-lab.princeton.edu/majumdar/">Anirudha&nbsp;Majumdar</a></span>,
            <span class="author-block"><a href="https://irom-lab.princeton.edu/majumdar/">Felix&nbsp;Heide</a></span>
          </div>
        </div>

        <!-- ===== RESOURCE ICONS ===== -->
        <div class="publication-links">
          <a href="static/paper.pdf" class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener"> <!--TODO: upload paper pdf-->
            <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
          </a>
          <a href="https://arxiv.org/abs/2505.07728" class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener"> <!--TODO: fix arxiv link-->
            <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
          </a>
          <a href="#" class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener">
            <span class="icon"><i class="fab fa-github"></i></span><span>Code (Coming Soon!)</span>
          </a>
          <a href="https://youtu.be/unTqsgAVE1o" class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener"> <!--TODO: fix video link-->
            <span class="icon"><i class="fab fa-youtube"></i></span><span>Video</span>
          </a>
        </div>

        <!-- ===== LAB LOGO ===== -->
        <figure class="image is-inline-block lab-logo">
          <img src="static/pci-icon.png" alt="PCI Lab logo" style="max-width:150px;">
        </figure>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <figure class="image is-inline-block lab-logo">
          <img src="static/irom-icon.png" alt="IROM Lab logo" style="max-width:150px;">
        </figure>
      </div>
    </div>
  </section>

  <!-- ===== OVERVIEW ===== -->
  <section id="overview" class="section">
    <div class="container is-max-desktop content-container has-text-centered">
      <video class="overview-video" autoplay loop muted playsinline poster="static/thumbnails/overview.jpg"> <!--TODO: update poster-->
        <source src="static/Anchor.mov" type="video/mp4" />
        Your browser does not support the video tag.
      </video>
      <p style="margin-top:0rem;">In autonomous driving, we aim to achieve both (1) <strong>fast online planning</strong> with a modularized differentiable end-to-end (e2e) architecture, and (2) <strong>human-like reasoning process</strong> with a vision-language model (VLM). Our key idea is to distill the reasoning process and commonsense knowledge from a VLM to the e2e driving model.</p>
    </div>
  </section>

  <!-- ===== ABSTRACT ===== -->
  <section id="abstract" class="section">
    <div class="container is-max-desktop content-container">
      <h2 class="title is-3 has-text-centered">Abstract</h2>
      <p>
        While autonomous driving (AD) stacks struggle with decision making under partial observability and real-world complexity, human drivers are capable of applying commonsense reasoning to make near-optimal decisions with limited information. Recent work has attempted to leverage finetuned Vision-Language Models (VLMs) for trajectory planning at inference time to emulate human behavior. Despite their success in benchmark evaluations, these methods are often impractical to deploy (a 70B parameter VLM inference at merely 8 tokens per second requires more than 160G of memory), and their monolithic network structure prohibits safety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for autonomous DrIving (VERDI), a training-time framework that distills the reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI augments modular differentiable end-to-end (e2e) AD models by aligning intermediate module outputs at the perception, prediction, and planning stages with text features explaining the driving reasoning process produced by VLMs. By encouraging alignment in latent space, VERDI enables the modular AD stack to internalize structured reasoning, without incurring the inference-time costs of large VLMs. We demonstrate the effectiveness of our method on the NuScenes dataset and find that VERDI outperforms existing e2e methods that do not embed reasoning by 10% in in \( \ell_2 \) distance, while maintaining high inference speed.
      </p>
      <br>
      <video id="summary-video" class="shadow" controls preload="metadata" width="100%" poster="static/thumbnails/talk_video.jpg"> <!--TODO: update-->
        <source src="static/videos/talk_video.mp4" type="video/mp4" /> <!--TODO: update-->
        Your browser does not support the video tag.
      </video>
    </div>
  </section>

 
<!--TODO: paste in our code?-->

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{
  }</code></pre> <!--TODO-->
    </div>
  </section>
  <br>
  <center class="is-size-10">
    The website design was adapted from <a href="https://nerfies.github.io" class="external-link"><span
        class="dnerf">Nerfies</span></a>.
  </center>
  <br>

  <!-- ===== SCRIPTS ===== -->
  <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.js"></script>
  <script>
    $(function(){
      // Init carousels (only those still using .task-carousel wrappers)
      $('.task-carousel').slick({
        slidesToShow:4,
        slidesToScroll:1,
        infinite:false,
        arrows:true,
        dots:false,
        lazyLoad:'ondemand',
        touchMove:true,
        responsive:[
          {breakpoint:1024,settings:{slidesToShow:3}},
          {breakpoint:768, settings:{slidesToShow:2}},
          {breakpoint:480, settings:{slidesToShow:1}}
        ]
      });

      // Horizontal track‑pad scroll → carousel navigation; let vertical scroll bubble up
      $('.task-carousel').on('wheel',function(e){
        const deltaX = e.originalEvent.deltaX;
        const deltaY = e.originalEvent.deltaY;
        if(Math.abs(deltaX) > Math.abs(deltaY)){
          e.preventDefault();
          $(this).slick(deltaX < 0 ? 'slickPrev' : 'slickNext');
        }
      });
    });
  </script>
</body>
</html>